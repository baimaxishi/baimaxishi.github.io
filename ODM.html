<!DOCTYPE html>
<!-- saved from url=(0033)https://mxu34.github.io/PromptDT/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content=" Online Decision MetaMor-phFormer">
    <meta name="author" content="Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua B. Tenenbaum and Chuang Gan">

    <title> Online Decision MetaMor-phFormer</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="./ODM/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="./ODM/offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2> Online Decision MetaMor-phFormer</h2>
    <h3>ICLR 2023</h3>
    <hr>
    <p class="authors">
        <a href="https://mxu34.github.io/">Mengdi Xu</a>,
        Yikang Shen,
        <a href="https://shunzh.github.io/">Shun Zhang</a>,
        <a href="http://jackhaha363.github.io/">Yuchen Lu</a>,
        <a href="https://www.meche.engineering.cmu.edu/directory/bios/zhao-ding.html">Ding Zhao</a>,
        <a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a> and
        <a href="http://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/pdf/2206.13499.pdf">Paper</a>
        <a class="btn btn-primary" href="https://github.com/mxu34/prompt-dt">Code</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <!-- <div class="vcontainer">
            <iframe class='video' src="" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div> -->
        <!-- <hr> -->
        <p>
            Human can leverage prior experience and learn novel tasks from a handful of demonstrations. 
            In contrast to offline meta-reinforcement learning, which aims to achieve quick adaptation through better algorithm design, we investigate the effect of architecture inductive bias on the few-shot learning capability. 
            We propose the Prompt-based Decision Transformer (Prompt-DT), which leverages the sequential modeling ability of Transformer architecture and the prompt framework to achieve few-shot adaptation in offline RL. 
            We design the trajectory prompt, which contains segments of the few-shot demonstrations, and encodes task-specific information to guide policy generation. 
            Our experiments in several Mujoco control tasks show that Prompt-DT is a strong few-shot learner without any extra finetuning on unseen target tasks. 
            Prompt-DT outperforms its variants and strong meta offline RL baselines by a large margin with a trajectory prompt containing only a few timesteps. 
            Prompt-DT is also robust to prompt length changes and can generalize to out-of-distribution (OOD) environments. 
        </p>

    </div>

    <br>

    <div class="section">
        <h2>Prompt-DT Architecture</h2>
        <hr>
        <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="./ODM/PromptDT.png" style="width:100%">
            </div>
        </div>
        <p>
            Our Prompt-DT architecture is built on <a href="https://sites.google.com/berkeley.edu/decision-transformer"> Decision Transformer</a> [Chen et al., 2021] and solves the offline few-shot RL problem through the lens of a prompt-augmented sequence-modeling problem.  
            The proposed trajectory prompt allows minimal architecture change to the Decision Transformer for generalization.
            For each task at training and testing time, Prompt-DT takes as input both the trajectory prompt obtained from expert demonstrations and the most recent context history.
            The data pair at each timestep is a 3-tuple (including state, action, and reward-to-go). 
            Prompt-DT autoregressively outputs actions at heads corresponding to state tokens in the input sequence.
        </p>
    </div>

    <br>

    <div class="section">
        <h2>Few-Shot Policy Generalization to In-distribution Tasks</h2>
        <hr>
        <p>
            We evaluate Prompt-DT in five environments that are widely used in offline meta-RL literature, including Cheetah-dir, Cheetah-vel, Ant-dir, Dial, and Metaworld-reach-v2.
            We compare Prompt-DT with Prompt-based Behavior Cloning (Prompt-MT-BC) to ablate the effect of reward-to-go tokens, Multi-task Offline RL (MT-ORL) to ablate the efficiency of our proposed trajectory prompts, Multi-task Behavior Cloning (MT-BC-Finetune), and <a href="https://proceedings.mlr.press/v139/mitchell21a.html">Meta-Actor Critic with Advantage Weighting (MACAW)</a> [Mitchell et al., 2021].
            <!-- <br> -->
            We find that Prompt-DT achieves high episodic accumulated rewards in never-before-seen tasks across environments by matching the task-specific information stored in a short trajectory prompt.        </p>
        <!-- <div class="col justify-content-center text-center">
            <div class="col-sm-12">
                <img src="img/main_final.png" style="width:100%">
            </div>
            
        </div> -->


        <div class="row justify-content-center text-center">
            <div class="col-sm-4">
                <h5>Cheetah-dir</h5>
                <img src="./ODM/cheetah-dir-prompt-dt-annotate.gif" style="width:100%">
                <h5>Prompt-DT</h5>
            </div>
            <div class="col-sm-4">
                <h5>Cheetah-vel</h5>
                <img src="./ODM/cheetah-vel-prompt-dt-annotate.gif" style="width:100%">
                <h5>Prompt-DT</h5>
            </div>
            <div class="col-sm-4">
                <h5>Meta-World pick-place-v2</h5>
                <img src="./ODM/metaworld-pick-place-prompt-dt.gif" style="width:100%">
                <h5>Prompt-DT</h5>
            </div>

            <hr>

            <div class="col-sm-4">
                <img src="./ODM/cheetah-dir-mt-orl-annotate.gif" style="width:100%">
                <h5>MT-ORL</h5>
            </div>
            <div class="col-sm-4">
                <img src="./ODM/cheetah-vel-mt-orl-annotate.gif" style="width:100%">
                <h5>MT-ORL</h5>
            </div>
            <div class="col-sm-4">
                <img src="./ODM/metaworld-pick-place-mt-orl.gif" style="width:100%">
                <h5>MT-ORL</h5>
            </div>
        </div>

    </div>

    <br>





    <div class="section">
        <h2>Few-Shot Policy Generalization to Out-of-distribution Tasks</h2>
        <hr>
        <div class="row justify-content-left">
            
            <div class="col-sm-7">
                <br>
                <p>
                    We desire to test whether trajectory prompts enable the extrapolation ability when handling tasks with goals out of the training ranges. 
                    We sample 8 training tasks in Ant-dir and 3 testing tasks, two of which have indexes smaller than the minimum task index and one larger than the maximum. 
                    The task index is proportional to the desired direction angle. 
                    <!-- <br> -->
                    We find that Prompt-DT still performs better than baselines with no prompt augmentations.
                </p>
            </div>
            <!-- <div class="col-sm-1">
            </div> -->
            <div class="col-sm-5">
                <img src="./ODM/Ant-dir-ood-1441-rebuttal.png" style="width:100%">
            </div>
        </div>

    </div>

    <br>

    <div class="section">
        <h2>Sensitivity to Prompt Quantity and Quality</h2>
        <hr>
        <div class="row justify-content-left">
                <p>
                    In practice, there may exist a limited amount of high-quality demonstrations for each test task, or the demonstrations may contain trajectories with heterogeneous quality. 
                    Our experiments show that, with trajectory prompt sampled from expert demonstrations and expert training dataset, Prompt-DT is not sensitive to the prompt quantity and can successfully extract task-specific information even with prompts containing only a few timesteps.
                    We conduct an ablation study in Cheetah-vel for prompt quality.
                    We find that Prompt-DT could adjust its generated actions according to the given trajectory prompt when training with expert data or medium data.
                    However, when training with random data, only feeding Prompt-DT expert or medium trajectory prompts does not help improve the generalization ability.  
                </p>
            <div class="col-sm-5">
                <br>
                <img src="./ODM/prompt_quantity_table.png" style="width:100%">
            </div>
            <div class="col-sm-7">
                <img src="./ODM/Ablation_Prompt_Quality_cheetah_vel.png" style="width:100%">
            </div>
        </div>

    </div>


    <br>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{xu2022prompt,
                title={Prompting Decision Transformer for Few-Shot Policy Generalization},
                author={Xu, Mengdi and Shen, Yikang and Zhang, Shun and Lu, Yuchen 
                and Zhao, Ding and Tenenbaum, B. Joshua and Gan, Chuang},
                booktitle={Thirty-ninth International Conference on Machine Learning},
                year={2022}
            }
        </div>
    </div>

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="https://mxu34.github.io/">Mengdi Xu</a>. 
            This page is modified based on <a href="https://yilundu.github.io/gem/">here</a>.
        </p>
    </footer>




    

</div>


<script src="./ODM/jquery-3.5.1.slim.min.js.下载" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="./ODM/popper.min.js.下载" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="./ODM/bootstrap.min.js.下载" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>


</body></html>